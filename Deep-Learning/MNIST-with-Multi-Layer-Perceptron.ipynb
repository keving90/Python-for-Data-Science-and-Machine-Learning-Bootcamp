{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the MNIST dataset, which is a dataset of handwritten digits. We will build a multi-layer perceptron (deep learning model) based on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('/tmp/data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mnist is stored as a special tensorflow dataset object\n",
    "type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy array\n",
    "mnist.train.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are non-zero decimal numbers in this numpy array. They represent the darkness of the pixels in a certain array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.02352941,  0.76470596,  0.99607849,  1.        ,  0.93725497,\n",
       "         0.1137255 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.02352941,\n",
       "         0.63921571,  0.99607849,  0.99607849,  0.68627453,  0.21568629,\n",
       "         0.01176471,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.09019608,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.47450984,\n",
       "         0.99607849,  0.99607849,  0.58039218,  0.03137255,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.54901963,\n",
       "         0.92941183,  0.43921572,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.16862746,  0.96078438,\n",
       "         0.99607849,  0.94901967,  0.01568628,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.20784315,  0.92549026,\n",
       "         0.99607849,  0.43921572,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.64313728,  0.99607849,\n",
       "         0.98039222,  0.25098041,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.66274512,  0.99607849,\n",
       "         0.97647065,  0.1254902 ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.83921576,  0.99607849,\n",
       "         0.69803923,  0.        ,  0.        ,  0.01176471,  0.03529412,\n",
       "         0.03529412,  0.03529412,  0.17254902,  0.90980399,  0.99607849,\n",
       "         0.64705884,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.83921576,  0.99607849,\n",
       "         0.72549021,  0.        ,  0.21176472,  0.72941178,  0.99607849,\n",
       "         0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.90588242,\n",
       "         0.16862746,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.37647063,  0.99607849,\n",
       "         0.98039222,  0.74509805,  0.98823535,  0.99607849,  0.99607849,\n",
       "         0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.50980395,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.49411768,  0.99607849,\n",
       "         0.99607849,  0.99607849,  0.99607849,  0.78039223,  0.46274513,\n",
       "         0.56862748,  0.99607849,  0.99607849,  0.99607849,  0.30588236,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.10980393,  0.47058827,\n",
       "         0.47058827,  0.47058827,  0.1254902 ,  0.01176471,  0.        ,\n",
       "         0.47450984,  0.99607849,  0.99607849,  0.76470596,  0.01568628,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.43137258,  0.99607849,  0.99607849,  0.33333334,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.41960788,  0.99607849,  0.98823535,  0.19215688,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.82352948,  0.99607849,  0.71764708,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.05882353,\n",
       "         0.87843144,  0.99607849,  0.27843139,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.27058825,\n",
       "         0.99607849,  0.97647065,  0.20784315,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.84705889,\n",
       "         0.99607849,  0.92549026,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.87450987,\n",
       "         0.99607849,  0.74509805,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.87450987,\n",
       "         0.99607849,  0.73333335,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.87450987,\n",
       "         0.99607849,  0.97647065,  0.49411768,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.44705886,\n",
       "         0.9333334 ,  0.99607849,  0.48627454,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[2].reshape(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = mnist.train.images[2].reshape(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x114eef470>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCD+QFOEHkiL8QFKEH0iK8ANJEX4gqf8HAIELZI9qhowAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a25c61978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imshow() will show a a two dimensional matrix based off some color scale\n",
    "plt.imshow(sample, cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Describes how quickly we handle the cost function\n",
    "# How quickly we want to apply the optimization function\n",
    "# Lower rate -> higher possibility for accurate training results at the cost of longer computation time\n",
    "learning_rate = 0.001\n",
    "\n",
    "# How many training cycles we will go through\n",
    "training_epocs = 15\n",
    "\n",
    "# Size of the batches of training data\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network parameters that directly define what our neural network will look like \n",
    "n_classes = 10\n",
    "n_samples = mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What we expect the input to look like\n",
    "# We're dealing with 28x28 arrays, but we're dealing with a flattened version\n",
    "n_input = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose how many neurons we want in the 2 hidden layers\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we recieve the input data array and send it to the first hidden layer. Then, the data will begin to have a weight attached to it between the layers. We will also add a bias. Then, this cycle continues to the next hidden layer until the final output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More hidden layers means the model will take longer to run, but it will also be more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One the transformed data that has been multiplied by these weights has reached the output layer, we need to evaluate it. We will use a loss function (cost function) to evaluate how far off we are from the desired result (How many classes did we get correct?). We will apply an optimization function to try and minimize the cost by adjusting the weight values accordingly across the entire network (Adam Optimizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for our multi-layer perceptron\n",
    "def multilayer_perceprton(x, weights, biases):\n",
    "    \"\"\"\n",
    "    x: Placeholder for data input\n",
    "    weights: Dictionary of weights\n",
    "    biases: Dict of bias values\n",
    "    \"\"\"\n",
    "    # First hidden layer with RELU Activation\n",
    "    # Use matrix multiplication on weights and x, then add in the bias (X * W + B)\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # RELU(X * W + B) = RELU -> f(x) = max(0, x)\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    # Second hidden layer\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # Last output layer\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    \n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow has a graph object that can become aware of all the states of the variables. A variable is a modifiable tensor living in tensorflow's graph interacting operations. It can be used and modified by the computation. Generally, the model parameters are variables, so we wil use `tf.Variable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    # Matrix of normally distributed random values \n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    \n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set 2 placeholders for x and y\n",
    "x = tf.placeholder('float', [None, n_input])\n",
    "y = tf.placeholder('float', [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct the model\n",
    "pred = multilayer_perceprton(x, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define out cost and optimization functions\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pass in a number, and it will return a tuple of a batch of n samples of the training data (10 in this case)\n",
    "# It produces n samples of training data\n",
    "t = mnist.train.next_batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tuple has length 2\n",
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An element in the tuple has length n (here, n = 1)\n",
    "# This element is a numpy array\n",
    "len(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 784)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of `mnist.train.next_batch()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Xsamp is a sample\n",
    "# ysame is the actual label\n",
    "Xsamp, ysamp = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a20875b38>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2077b668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Xsamp.reshape(28, 28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to training the model and running the session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lauch an interactive session. We don't need to use a context manager in this case.\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Cost: 173.4777\n",
      "Epoch: 2, Cost: 42.3939\n",
      "Epoch: 3, Cost: 26.6762\n",
      "Epoch: 4, Cost: 18.7960\n",
      "Epoch: 5, Cost: 13.5797\n",
      "Epoch: 6, Cost: 10.1510\n",
      "Epoch: 7, Cost: 7.6300\n",
      "Epoch: 8, Cost: 5.7487\n",
      "Epoch: 9, Cost: 4.3363\n",
      "Epoch: 10, Cost: 3.3723\n",
      "Epoch: 11, Cost: 2.6040\n",
      "Epoch: 12, Cost: 1.9242\n",
      "Epoch: 13, Cost: 1.5034\n",
      "Epoch: 14, Cost: 1.1652\n",
      "Epoch: 15, Cost: 0.9944\n",
      "Model has completed 15 Epochs of training.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# Recall: training_epochs = 5\n",
    "for epoch in range(training_epocs):\n",
    "    # Cost\n",
    "    avg_cost = 0.0\n",
    "    \n",
    "    # Get the actual batch size\n",
    "    total_batch = int(n_samples / batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size) # batch_size = 100\n",
    "        \n",
    "        # Use the feed_dict for the optimization and loss values\n",
    "        # Use an underscore when you don't need a value when tuple unpacking\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "        \n",
    "        # Compute the average loss\n",
    "        avg_cost += c / total_batch\n",
    "        \n",
    "    print('Epoch: {}, Cost: {:.4f}'.format(epoch+1, avg_cost))\n",
    "\n",
    "print('Model has completed {} Epochs of training.'.format(training_epocs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Evaluate the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check how many predictions are equal to the test\n",
    "# tf.equal() is essentially an equal check. It returns a boolean array of where x == y (element-wise)\n",
    "# tf.argmax() returns the index with the largest value across axes of a tensor.\n",
    "correct_predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal:0\", shape=(?,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "print(correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use tf.cast() to cast our booleans back into a tensor of floating poinr values in order to get an average\n",
    "correct_predictions = tf.cast(correct_predictions, 'float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Cast_2:0\", shape=(?,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use a mean() function to grab the mean values across the tensor\n",
    "accuracy = tf.reduce_mean(correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate our accuracy\n",
    "mnist.test.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.32941177,  0.72549021,  0.62352943,\n",
       "        0.59215689,  0.23529413,  0.14117648,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.8705883 ,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.9450981 ,  0.77647066,  0.77647066,  0.77647066,  0.77647066,\n",
       "        0.77647066,  0.77647066,  0.77647066,  0.77647066,  0.66666669,\n",
       "        0.20392159,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.26274511,  0.44705886,\n",
       "        0.28235295,  0.44705886,  0.63921571,  0.89019614,  0.99607849,\n",
       "        0.88235301,  0.99607849,  0.99607849,  0.99607849,  0.98039222,\n",
       "        0.89803928,  0.99607849,  0.99607849,  0.54901963,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.06666667,  0.25882354,  0.05490196,  0.26274511,\n",
       "        0.26274511,  0.26274511,  0.23137257,  0.08235294,  0.92549026,\n",
       "        0.99607849,  0.41568631,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.32549021,  0.99215692,  0.81960791,  0.07058824,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.08627451,  0.91372555,\n",
       "        1.        ,  0.32549021,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.50588238,  0.99607849,  0.9333334 ,  0.17254902,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.23137257,  0.97647065,\n",
       "        0.99607849,  0.24313727,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.52156866,  0.99607849,  0.73333335,  0.01960784,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.03529412,  0.80392164,\n",
       "        0.97254908,  0.227451  ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.49411768,  0.99607849,  0.71372551,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.29411766,  0.98431379,\n",
       "        0.94117653,  0.22352943,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.07450981,  0.86666673,  0.99607849,  0.65098041,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.01176471,  0.7960785 ,  0.99607849,\n",
       "        0.8588236 ,  0.13725491,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.14901961,  0.99607849,  0.99607849,  0.3019608 ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.12156864,  0.87843144,  0.99607849,\n",
       "        0.45098042,  0.00392157,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.52156866,  0.99607849,  0.99607849,  0.20392159,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.2392157 ,  0.94901967,  0.99607849,\n",
       "        0.99607849,  0.20392159,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.47450984,  0.99607849,  0.99607849,  0.8588236 ,  0.15686275,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.47450984,  0.99607849,\n",
       "        0.81176478,  0.07058824,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9447"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get our accuracy\n",
    "accuracy.eval({x: mnist.test.images, y: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy is good, but, with the advanced capabilities of deep learning and tensorflow, it could be much more accurate. Most real deep learning models use way more epochs, but they can take much longer too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
